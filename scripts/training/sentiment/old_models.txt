# Model 1 - LSTM with dropout and 2 hidden layers
# model = Sequential()
# model.add(Word2Vec(max_sentence_len))
# model.add(LSTM(512, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, use_bias=True))
# model.add(Dropout(0.2))
# model.add(Dense(512, activation='relu'))
# model.add(Dense(512, activation='relu'))
# model.add(Dense(5, activation='softmax'))
# optimizer = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)
# model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
# model.fit(x=x_train_padded, y=y_train, batch_size=16, epochs=20, verbose=1, shuffle=True, validation_data=(x_validation_padded, y_validation), callbacks=[tb_callback])

# Model 2 - LSTM with dropout and 10 hidden layers
# model = Sequential()
# model.add(Word2Vec(max_sentence_len))
# model.add(LSTM(256, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, use_bias=True))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(64, activation='relu'))
# model.add(Dense(5, activation='softmax'))
# # optimizer = tf.keras.optimizers.Adam(lr=1e-4, decay=1e-5)
# optimizer = tf.keras.optimizers.Adam(lr=1e-4)
# model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
# model.fit(x=x_train_padded, y=y_train, batch_size=32, epochs=20, verbose=1, shuffle=True, validation_data=(x_validation_padded, y_validation), callbacks=[tb_callback])

# # Model 3 - LSTM with dropout and 5 hidden layers -> 2nd best performing
# model = Sequential()
# model.add(Word2Vec(max_sentence_len))
# model.add(LSTM(256, activation='tanh', recurrent_activation='sigmoid', recurrent_dropout=0, unroll=False, use_bias=True))
# model.add(BatchNormalization())
# model.add(Dropout(0.25))
# model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))
# model.add(Dropout(0.25))
# model.add(Dense(128, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))
# model.add(Dropout(0.25))
# model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))
# model.add(Dropout(0.25))
# model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))
# model.add(Dropout(0.25))
# model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)))
# model.add(Dropout(0.25))
# model.add(Dense(3, activation='softmax'))
# optimizer = tf.keras.optimizers.Adam(lr=1e-4)
# model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
# model.fit(x=x_train_padded, y=y_train, batch_size=128, epochs=20, verbose=1, shuffle=True, validation_data=(x_validation_padded, y_validation), callbacks=[tb_callback])